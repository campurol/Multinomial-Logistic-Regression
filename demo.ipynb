{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "This notebook is intended to showcase how to use the MNL (Multinomial Logistic Regression) model to predict the booking probability for each option within a session.\n",
    "\n",
    "One can find the sample training and testing data under the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model and all the auxiliary functions\n",
    "from MNL import *\n",
    "from MNL_plus import *\n",
    "from Mint import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-r5ol5wj4 because the default path (/home/jovyan/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15,6\n",
    "rcParams['figure.dpi'] = 100\n",
    "rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save output into txt file\n",
    "from contextlib import redirect_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    #'MNL_features': MNL_features,\n",
    "    \n",
    "    # when absent, by default, use all the features within the training data\n",
    "    #'alter_features': MNL_features,\n",
    "    #'session_features'\n",
    "    \n",
    "    # options: BinaryCrossEntropy, MaxLogLikelihood\n",
    "    #'loss':  'MaxLogLikelihood',\n",
    "    'loss':  'MaxLogLikelihood',\n",
    "    \n",
    "    'expand': False,\n",
    "    \n",
    "    'optimizer': 'Adam',  # options:  Adam, RMSprop, SGD, LBFGS.\n",
    "    # Adam would converge much faster\n",
    "    # LBFGS is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes).\n",
    "    # If it doesnâ€™t fit in memory try reducing the history size, or use a different algorithm.\n",
    "    # By default, history_size == 100\n",
    "    'learning_rate': 0.1, # Applicable to Adam, SGD, and LBFGS\n",
    "    # The learning_rate parameter seems essential to LBFGS, which converges in two epochs.\n",
    "    #  So far, learning_rate == 0.1 seems to be ok for LBFGS\n",
    "    \n",
    "    #'momentum': 0.9,  # applicable to SGD, RMSprop\n",
    "    'momentum': 0.01,  # applicable to SGD, RMSprop\n",
    "    \n",
    "    # The resulting model seems to be more balanced, i.e. no extreme large/small weights,\n",
    "    #  although one might not have the most ideal performance, i.e. high top_5_rank etc.\n",
    "    'weight_decay': 0, # Applicable to Adam, RMSprop and SGD\n",
    "    \n",
    "\n",
    "    # indicates the number of sessions included in each batch\n",
    "    'batch_size': 500,\n",
    "    \n",
    "    #maximum number of epochs\n",
    "    'epochs': 20,\n",
    "    \n",
    "    #tolerance for early stopping\n",
    "    'early_stop_min_delta': 1e-4,\n",
    "    'patience': 5,\n",
    "    \n",
    "    #if able to use GPU (unfortunately I am not able to do this)\n",
    "    'gpu': False,  # luckily, running on GPU is faster than CPU in this case.\n",
    "    \n",
    "    # level of logging, 0: no log,  1: print epoch related logs;  2: print session related logs\n",
    "    'verbose': 1,\n",
    "    \n",
    "    # Adding the regularization degredates the performance of model\n",
    "    #   which might suggests that the model is still underfitting, not overfitting.\n",
    "    'l1_loss_weight': 0,  # e.g. 0.001 the regularization that would marginalize the weights\n",
    "    'l2_loss_weight': 0,\n",
    "    \n",
    "    # flag indicates whether to save gradients during the training\n",
    "    'save_gradients': False\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for reproduceability\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "\n",
    "df_train = pd.read_csv('data/train_SINBKK_RT_B.csv')\n",
    "\n",
    "# Create a brand-new model\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "model_tuple, loss_list = run_training(df_training=df_train, train_config=TRAIN_CONFIG)\n",
    "\n",
    "# Continue training on the existing model\n",
    "model_tuple, loss_list = run_training(df_training=df_train, train_config=TRAIN_CONFIG, model_tuple=model_tuple)\n",
    "\n",
    "\n",
    "# unzip the tuple\n",
    "(model, loss, optimizer) = model_tuple\n",
    "\n",
    "\n",
    "# plot the evolution of loss\n",
    "plot_loss(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Stats\n",
    "\n",
    "In this section, we calculate some statistics in the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with the testing data\n",
    "# Calculate the probability and the trank of the chosen alternative\n",
    "train_stats = validate(model, df_train, TRAIN_CONFIG)\n",
    "train_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the testing results into a list of KPIs, such as:\n",
    "\n",
    "- *mean_probability*: the average probability of the predicted alternative among all sessions\n",
    "\n",
    "\n",
    "- *top_5_rank_quantile*: the percentile of sessions where the probability of the predicted alternative is among the top 5.\n",
    "\n",
    "\n",
    "- *AIC*: Akaike Information Criterion, which offers an estimate of the relative information lost when a given model is used to represent the process that generated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_KPIs(train_stats, len(TRAIN_CONFIG['MNL_features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation with expand option \n",
    "\n",
    "In this section, I show how to use the expand option. You will see that there are three datasets this time (take a look at the dummy do file in the data folder to see how they are constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id=['session_id']\n",
    "alter_id=['alter_id']\n",
    "alter_features=\n",
    "session_features=\n",
    "choice_groups=\n",
    "session_alter_features="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define choice-alternative function\n",
    "def session_alter(data):\n",
    "    return data['reco_contains_mh']*data['deptime_outbound_sin4p']+data['deptime_outbound_cos4p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nmlogit_config\t import *\n",
    "config['session_id']='session_id'\n",
    "config['alter_id']='alter_id'\n",
    "config['alter_features']=['reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od']\n",
    "config['session_features']=['deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p']\n",
    "config['choice_groups']=['group','group2']\n",
    "config['session_alter_features']=[]\n",
    "config['extra_choice_features']=[]\n",
    "config['session_alter_fn']=[]\n",
    "config['drop_vars']=[]\n",
    "config['batch_size']=7788\n",
    "config['loss']='BinaryCrossEntropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = pd.read_csv('data/train_SINBKK_RT_B_choices.csv')[['session_id','alter_id','choice','group','group2']]\n",
    "alter_data = pd.read_csv('data/train_SINBKK_RT_B_alter_id.csv')\n",
    "session_data = pd.read_csv('data/train_SINBKK_RT_B_session_id.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 15\n",
      "========================\n",
      "All features: ['reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od', 'deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p']\n",
      "========================\n",
      "Alternative Features: ['reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od']\n",
      "========================\n",
      "Session Features: ['deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p']\n",
      "========================\n",
      "Loaded but dropped features: []\n",
      "========================\n",
      "Group IDs: ['group', 'group2']\n",
      "========================\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 0  loss: 0.07682089745409494 best_loss: 1000000000000000.0\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 1  loss: 0.07681942857121615 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 2  loss: 0.07681929676373608 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 3  loss: 0.07681954899918259 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 4  loss: 0.07681944910636176 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 5  loss: 0.07681912202411582 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 6  loss: 0.07681883490168719 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 7  loss: 0.07681871806682664 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 8  loss: 0.0768187310626754 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 9  loss: 0.07681873857298459 best_loss: 0.07682089745409494\n",
      "Batch_id: 0/1: Size=7788 expanded to 486750\n",
      "epoch: 10  loss: 0.07681865038169515 best_loss: 0.07682089745409494\n",
      "Early stopping!  epoch: 10 min_delta: 0.0001  patience: 10\n",
      "Final epoch: 10  loss: 0.07681865038169515\n"
     ]
    }
   ],
   "source": [
    "model_tuple, loss_list = run_training(df_training=choices, train_config=config, alter_data=alter_data, session_data=session_data)\n",
    "(model, loss, optimizer) = model_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'grad_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-7e0048378172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Parameter' object has no attribute 'grad_tensor'"
     ]
    }
   ],
   "source": [
    "model.get_params().grad_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient2=model.get_params().grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        gradient = [item.grad for item in model.parameters()]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNL' object has no attribute 'grad_tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-941a16e90cf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    576\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MNL' object has no attribute 'grad_tensor'"
     ]
    }
   ],
   "source": [
    "model.grad_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2265e-06,  1.0112e-05,  0.0000e+00, -6.1684e-06,  0.0000e+00,\n",
       "         -8.9087e-07,  0.0000e+00, -6.8508e-18,  1.2993e-18, -3.6253e-19,\n",
       "          8.1315e-19, -2.1176e-18,  1.1922e-18,  4.2352e-20,  6.8483e-19]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "x=np.random.rand(5000)\n",
    "y=np.random.rand(5000)\n",
    "z=np.random.rand(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where((y>x) & (x<y),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[3]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 5, 0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(choices['choice']==0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = pd.read_csv('data/train_SINBKK_RT_B_choices.csv')[['session_id','alter_id','choice','group','group2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-17c8690424c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_testing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malter_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malter_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "test_results = test_model(model=model, df_testing=choices, train_config=config, alter_data=alter_data, session_data=session_data)\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = pd.read_csv('data/train_SINBKK_RT_B_choices.csv')[['session_id','alter_id','choice','group','group2']]\n",
    "# Test the model with the testing data\n",
    "# Calculate the probability and the trank of the chosen alternative\n",
    "train_stats = validate(model=model, df_testing=choices, train_config=config, alter_data=alter_data, session_data=session_data)\n",
    "train_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the testing results into a list of KPIs, such as:\n",
    "\n",
    "- *mean_probability*: the average probability of the predicted alternative among all sessions\n",
    "\n",
    "\n",
    "- *top_5_rank_quantile*: the percentile of sessions where the probability of the predicted alternative is among the top 5.\n",
    "\n",
    "\n",
    "- *AIC*: Akaike Information Criterion, which offers an estimate of the relative information lost when a given model is used to represent the process that generated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_KPIs(train_stats, len(config['MNL_features']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#with open('out2.txt', 'w') as f:\n",
    "#    with redirect_stdout(f):\n",
    "model_tuple, loss_list = run_training(df_training=choices, train_config=TRAIN_CONFIG, alter_data=alter_data, session_data=session_data)\n",
    "        #model_tuple, loss_list = run_training(df_training=choices2, train_config=TRAIN_CONFIG, alter_data=alter_data, session_data=session_data, model_tuple=model_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ger(gradient[0],gradient[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from operator import truediv\n",
    "\n",
    "b=model.get_params()\n",
    "gradient=model.get_params().grad\n",
    "hessian=torch.mm(torch.t(gradient),gradient)\n",
    "se = torch.sqrt(torch.diagonal(hessian,0))\n",
    "t=torch.div(b,se)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "x=torch.tensor(2.0, requires_grad=True)  \n",
    "z=torch.tensor(4.0, requires_grad=True)  \n",
    "y=x**2+z**3  \n",
    "y.backward()  \n",
    "x.grad  \n",
    "z.grad  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = torch.arange(1., 5.)\n",
    "v2 = torch.arange(1., 4.)\n",
    "torch.outer(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.outer(gradient,gradient)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=b.tolist()\n",
    "se=se.tolist()\n",
    "gradient=gradient.tolist()\n",
    "hessian=hessian.tolist()\n",
    "t=t.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=1-norm.cdf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diagonal(hessian).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params().grad[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1], [2],[3],[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2,0,4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(x==0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "            gradient = torch.stack([(item.grad ** 2).sum() for item in model.parameters()]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-a34a5938db7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Define a function that returns gradients of training loss using Autograd.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtraining_gradient_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Optimize weights using gradient descent.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: grad() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (np.tanh(x / 2.) + 1)\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    # Outputs probability of a label being true according to logistic model.\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights):\n",
    "    # Training loss is the negative log-likelihood of the training labels.\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(label_probabilities))\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "# Define a function that returns gradients of training loss using Autograd.\n",
    "training_gradient_fun = grad(training_loss)\n",
    "\n",
    "# Optimize weights using gradient descent.\n",
    "weights = np.array([0.0, 0.0, 0.0])\n",
    "print(\"Initial loss:\", training_loss(weights))\n",
    "for i in range(100):\n",
    "    weights -= training_gradient_fun(weights) * 0.01\n",
    "\n",
    "print(\"Trained loss:\", training_loss(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
