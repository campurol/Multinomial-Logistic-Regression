{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2018 Lisong Guo <lisong.guo@me.com>\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of\n",
    "this software and associated documentation files (the \"Software\"), to deal in\n",
    "the Software without restriction, including without limitation the rights to\n",
    "use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n",
    "the Software, and to permit persons to whom the Software is furnished to do so,\n",
    "subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n",
    "FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\n",
    "COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\n",
    "IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\n",
    "CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "This notebook is intended to showcase how to use the MNL (Multinomial Logistic Regression) model to predict the booking probability for each option within a session.\n",
    "\n",
    "One can find the sample training and testing data under the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model and all the auxiliary functions\n",
    "from MNL import *\n",
    "from MNL_plus import *\n",
    "from Mint import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15,6\n",
    "rcParams['figure.dpi'] = 100\n",
    "rcParams['savefig.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_CONFIG = {\n",
    "    #'MNL_features': MNL_features,\n",
    "    \n",
    "    # when absent, by default, use all the features within the training data\n",
    "    #'alter_features': MNL_features,\n",
    "    #'session_features'\n",
    "    \n",
    "    # options: BinaryCrossEntropy, MaxLogLikelihood\n",
    "    #'loss':  'MaxLogLikelihood',\n",
    "    'loss':  'BinaryCrossEntropy',\n",
    "    \n",
    "    'expand': False,\n",
    "    \n",
    "    'optimizer': 'Adam',  # options:  Adam, RMSprop, SGD, LBFGS.\n",
    "    # Adam would converge much faster\n",
    "    # LBFGS is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes).\n",
    "    # If it doesnâ€™t fit in memory try reducing the history size, or use a different algorithm.\n",
    "    # By default, history_size == 100\n",
    "    'learning_rate': 1e-3, # Applicable to Adam, SGD, and LBFGS\n",
    "    # The learning_rate parameter seems essential to LBFGS, which converges in two epochs.\n",
    "    #  So far, learning_rate == 0.1 seems to be ok for LBFGS\n",
    "    \n",
    "    #'momentum': 0.9,  # applicable to SGD, RMSprop\n",
    "    'momentum': 0,  # applicable to SGD, RMSprop\n",
    "    \n",
    "    # The resulting model seems to be more balanced, i.e. no extreme large/small weights,\n",
    "    #  although one might not have the most ideal performance, i.e. high top_5_rank etc.\n",
    "    'weight_decay': 0, # Applicable to Adam, RMSprop and SGD\n",
    "    \n",
    "    'epochs': 20,\n",
    "    'early_stop_min_delta': 1e-4,\n",
    "    'patience': 5,\n",
    "    \n",
    "    'gpu': False,  # luckily, running on GPU is faster than CPU in this case.\n",
    "    \n",
    "    # level of logging, 0: no log,  1: print epoch related logs;  2: print session related logs\n",
    "    'verbose': 1,\n",
    "    \n",
    "    # Adding the regularization degredates the performance of model\n",
    "    #   which might suggests that the model is still underfitting, not overfitting.\n",
    "    'l1_loss_weight': 0,  # e.g. 0.001 the regularization that would marginalize the weights\n",
    "    'l2_loss_weight': 0,\n",
    "    \n",
    "    # flag indicates whether to save gradients during the training\n",
    "    'save_gradients': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 17\n",
      "========================\n",
      "{'loss': 'BinaryCrossEntropy', 'expand': False, 'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'epochs': 20, 'early_stop_min_delta': 0.0001, 'patience': 5, 'gpu': False, 'verbose': 1, 'l1_loss_weight': 0, 'l2_loss_weight': 0, 'save_gradients': False, 'MNL_features': ['deptime_inbound_cos2p', 'deptime_inbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'price_elasticity', 'reco_contains_CX', 'reco_contains_MH', 'reco_contains_OD', 'reco_contains_PG', 'reco_contains_SQ', 'reco_contains_TG', 'reco_contains_VN', 'rescaled_reco_eft']}\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 692.8990890536323 best_loss: 1000000000000000.0\n",
      "epoch: 1  loss: 182.468878417873 best_loss: 692.8990890536323\n",
      "epoch: 2  loss: 179.89239988701556 best_loss: 182.468878417873\n",
      "epoch: 3  loss: 178.82548713288176 best_loss: 179.89239988701556\n",
      "epoch: 4  loss: 178.26495447766675 best_loss: 178.82548713288176\n",
      "epoch: 5  loss: 177.93150318686247 best_loss: 178.26495447766675\n",
      "epoch: 6  loss: 177.70852256815377 best_loss: 177.93150318686247\n",
      "epoch: 7  loss: 177.5447816356338 best_loss: 177.70852256815377\n",
      "epoch: 8  loss: 177.41677550798718 best_loss: 177.5447816356338\n",
      "epoch: 9  loss: 177.31219554987783 best_loss: 177.41677550798718\n",
      "epoch: 10  loss: 177.22387530108801 best_loss: 177.31219554987783\n",
      "epoch: 11  loss: 177.14738076443865 best_loss: 177.22387530108801\n",
      "epoch: 12  loss: 177.07985203610028 best_loss: 177.14738076443865\n",
      "epoch: 13  loss: 177.01937734441074 best_loss: 177.07985203610028\n",
      "epoch: 14  loss: 176.96463399585278 best_loss: 177.01937734441074\n",
      "epoch: 15  loss: 176.91467505032875 best_loss: 176.96463399585278\n",
      "epoch: 16  loss: 176.86879932300556 best_loss: 176.91467505032875\n",
      "epoch: 17  loss: 176.82647047050958 best_loss: 176.86879932300556\n",
      "epoch: 18  loss: 176.7872656468075 best_loss: 176.82647047050958\n",
      "epoch: 19  loss: 176.7508423082333 best_loss: 176.7872656468075\n",
      "Final epoch: 19  loss: 176.7508423082333\n",
      "Num features: 17\n",
      "========================\n",
      "{'loss': 'BinaryCrossEntropy', 'expand': False, 'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'epochs': 20, 'early_stop_min_delta': 0.0001, 'patience': 5, 'gpu': False, 'verbose': 1, 'l1_loss_weight': 0, 'l2_loss_weight': 0, 'save_gradients': False, 'MNL_features': ['deptime_inbound_cos2p', 'deptime_inbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'price_elasticity', 'reco_contains_CX', 'reco_contains_MH', 'reco_contains_OD', 'reco_contains_PG', 'reco_contains_SQ', 'reco_contains_TG', 'reco_contains_VN', 'rescaled_reco_eft']}\n",
      "========================\n",
      "Continue training...\n",
      "epoch: 0  loss: 176.71691634294612 best_loss: 1000000000000000.0\n",
      "epoch: 1  loss: 176.68524737455775 best_loss: 176.71691634294612\n",
      "epoch: 2  loss: 176.65562867756034 best_loss: 176.68524737455775\n",
      "epoch: 3  loss: 176.62788010113465 best_loss: 176.65562867756034\n",
      "epoch: 4  loss: 176.60184298557954 best_loss: 176.62788010113465\n",
      "epoch: 5  loss: 176.5773764201256 best_loss: 176.60184298557954\n",
      "epoch: 6  loss: 176.55435441932062 best_loss: 176.5773764201256\n",
      "epoch: 7  loss: 176.5326637396276 best_loss: 176.55435441932062\n",
      "epoch: 8  loss: 176.5122021507884 best_loss: 176.5326637396276\n",
      "epoch: 9  loss: 176.49287703749627 best_loss: 176.5122021507884\n",
      "epoch: 10  loss: 176.47460424729383 best_loss: 176.49287703749627\n",
      "epoch: 11  loss: 176.45730712672596 best_loss: 176.47460424729383\n",
      "epoch: 12  loss: 176.44091570436342 best_loss: 176.45730712672596\n",
      "epoch: 13  loss: 176.425365990342 best_loss: 176.44091570436342\n",
      "epoch: 14  loss: 176.410599369737 best_loss: 176.425365990342\n",
      "epoch: 15  loss: 176.39656207274209 best_loss: 176.410599369737\n",
      "epoch: 16  loss: 176.3832047086794 best_loss: 176.39656207274209\n",
      "epoch: 17  loss: 176.37048185379302 best_loss: 176.3832047086794\n",
      "epoch: 18  loss: 176.35835168491593 best_loss: 176.37048185379302\n",
      "epoch: 19  loss: 176.34677565263328 best_loss: 176.35835168491593\n",
      "Final epoch: 19  loss: 176.34677565263328\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set random seed for reproduceability\n",
    "np.random.seed(17)\n",
    "torch.manual_seed(17)\n",
    "\n",
    "df_train = pd.read_csv('data/train_SINBKK_RT_B.csv')\n",
    "\n",
    "# Create a brand-new model\n",
    "model_tuple, loss_list = run_training(df_training=df_train, train_config=TRAIN_CONFIG)\n",
    "\n",
    "# Continue training on the existing model\n",
    "model_tuple, loss_list = run_training(df_training=df_train, train_config=TRAIN_CONFIG, model_tuple=model_tuple)\n",
    "\n",
    "\n",
    "# unzip the tuple\n",
    "(model, loss, optimizer) = model_tuple\n",
    "\n",
    "\n",
    "# plot the evolution of loss\n",
    "plot_loss(loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "\n",
    "In this section, we test the trained model and calculate some performance benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of testing sessions: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>alter_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>price_elasticity</th>\n",
       "      <th>rescaled_reco_eft</th>\n",
       "      <th>reco_contains_MH</th>\n",
       "      <th>reco_contains_TG</th>\n",
       "      <th>reco_contains_PG</th>\n",
       "      <th>reco_contains_SQ</th>\n",
       "      <th>reco_contains_VN</th>\n",
       "      <th>...</th>\n",
       "      <th>reco_contains_OD</th>\n",
       "      <th>deptime_outbound_sin2p</th>\n",
       "      <th>deptime_outbound_sin4p</th>\n",
       "      <th>deptime_outbound_cos2p</th>\n",
       "      <th>deptime_outbound_cos4p</th>\n",
       "      <th>deptime_inbound_sin2p</th>\n",
       "      <th>deptime_inbound_sin4p</th>\n",
       "      <th>deptime_inbound_cos2p</th>\n",
       "      <th>deptime_inbound_cos4p</th>\n",
       "      <th>pred_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8710</th>\n",
       "      <td>1703033726</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>316.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.906308</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.915312</td>\n",
       "      <td>7.372771e-01</td>\n",
       "      <td>-4.027466e-01</td>\n",
       "      <td>-0.67559</td>\n",
       "      <td>1.721015e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8711</th>\n",
       "      <td>1703033726</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>216.2</td>\n",
       "      <td>3.997044</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.806445</td>\n",
       "      <td>0.953717</td>\n",
       "      <td>-0.591309</td>\n",
       "      <td>-0.300706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>6.587497e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8712</th>\n",
       "      <td>1703033726</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>216.2</td>\n",
       "      <td>5.497044</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.806445</td>\n",
       "      <td>0.953717</td>\n",
       "      <td>-0.591309</td>\n",
       "      <td>-0.300706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>2.114179e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8713</th>\n",
       "      <td>1703033726</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>216.2</td>\n",
       "      <td>7.330044</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.065403</td>\n",
       "      <td>0.130527</td>\n",
       "      <td>-0.997859</td>\n",
       "      <td>0.991445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>1.364682e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>1703033726</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>216.2</td>\n",
       "      <td>7.330044</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.806445</td>\n",
       "      <td>0.953717</td>\n",
       "      <td>-0.591309</td>\n",
       "      <td>-0.300706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>5.272160e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_id  alter_id  choice  price_elasticity  rescaled_reco_eft  \\\n",
       "8710  1703033726         0       1             316.2           0.000000   \n",
       "8711  1703033726         1       0             216.2           3.997044   \n",
       "8712  1703033726         2       0             216.2           5.497044   \n",
       "8713  1703033726         3       0             216.2           7.330044   \n",
       "8714  1703033726         4       0             216.2           7.330044   \n",
       "\n",
       "      reco_contains_MH  reco_contains_TG  reco_contains_PG  reco_contains_SQ  \\\n",
       "8710                 0                 1                 0                 0   \n",
       "8711                 1                 0                 0                 0   \n",
       "8712                 1                 0                 0                 0   \n",
       "8713                 1                 0                 0                 0   \n",
       "8714                 1                 0                 0                 0   \n",
       "\n",
       "      reco_contains_VN  ...  reco_contains_OD  deptime_outbound_sin2p  \\\n",
       "8710                 0  ...                 0                0.906308   \n",
       "8711                 0  ...                 0               -0.806445   \n",
       "8712                 0  ...                 0               -0.806445   \n",
       "8713                 0  ...                 0               -0.065403   \n",
       "8714                 0  ...                 0               -0.806445   \n",
       "\n",
       "      deptime_outbound_sin4p  deptime_outbound_cos2p  deptime_outbound_cos4p  \\\n",
       "8710               -0.766044               -0.422618               -0.642788   \n",
       "8711                0.953717               -0.591309               -0.300706   \n",
       "8712                0.953717               -0.591309               -0.300706   \n",
       "8713                0.130527               -0.997859                0.991445   \n",
       "8714                0.953717               -0.591309               -0.300706   \n",
       "\n",
       "      deptime_inbound_sin2p  deptime_inbound_sin4p  deptime_inbound_cos2p  \\\n",
       "8710              -0.915312           7.372771e-01          -4.027466e-01   \n",
       "8711               1.000000           1.224647e-16           6.123234e-17   \n",
       "8712               1.000000           1.224647e-16           6.123234e-17   \n",
       "8713               1.000000           1.224647e-16           6.123234e-17   \n",
       "8714               1.000000           1.224647e-16           6.123234e-17   \n",
       "\n",
       "      deptime_inbound_cos4p    pred_value  \n",
       "8710               -0.67559  1.721015e-01  \n",
       "8711               -1.00000  6.587497e-06  \n",
       "8712               -1.00000  2.114179e-06  \n",
       "8713               -1.00000  1.364682e-06  \n",
       "8714               -1.00000  5.272160e-07  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('data/test_SINBKK_RT_B.csv')\n",
    "\n",
    "test_results = test_model(model, df_test, TRAIN_CONFIG)\n",
    "\n",
    "test_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of testing sessions: 542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_id</th>\n",
       "      <th>session_size</th>\n",
       "      <th>num_chosen_choices</th>\n",
       "      <th>rank_of_chosen_one</th>\n",
       "      <th>prob_of_chosen_one</th>\n",
       "      <th>max_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1702036</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.027704</td>\n",
       "      <td>0.152154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1704193</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.033080</td>\n",
       "      <td>0.116134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17033132</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.274971</td>\n",
       "      <td>0.274971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17041139</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.072115</td>\n",
       "      <td>0.255718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17042962</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.100252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session_id  session_size  num_chosen_choices  rank_of_chosen_one  \\\n",
       "0     1702036            36                   1                12.0   \n",
       "1     1704193            60                   1                 8.0   \n",
       "2    17033132            60                   1                 1.0   \n",
       "3    17041139            60                   1                 5.0   \n",
       "4    17042962            60                   1                10.0   \n",
       "\n",
       "   prob_of_chosen_one  max_prob  \n",
       "0            0.027704  0.152154  \n",
       "1            0.033080  0.116134  \n",
       "2            0.274971  0.274971  \n",
       "3            0.072115  0.255718  \n",
       "4            0.036439  0.100252  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model with the testing data\n",
    "# Calculate the probability and the trank of the chosen alternative\n",
    "test_stats = validate(model, df_test, TRAIN_CONFIG)\n",
    "\n",
    "test_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the testing results into a list of KPIs, such as:\n",
    "\n",
    "- *mean_probability*: the average probability of the predicted alternative among all sessions\n",
    "\n",
    "\n",
    "- *top_5_rank_quantile*: the percentile of sessions where the probability of the predicted alternative is among the top 5.\n",
    "\n",
    "\n",
    "- *AIC*: Akaike Information Criterion, which offers an estimate of the relative information lost when a given model is used to represent the process that generated the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_num': 542,\n",
       " 'mean_session_size': 54.7029520295203,\n",
       " 'top_1_rank_quantile': 17.15867158671587,\n",
       " 'top_5_rank_quantile': 52.952029520295206,\n",
       " 'top_10_rank_quantile': 71.77121771217713,\n",
       " 'mean_rank_ratio': 0.17871061674879246,\n",
       " 'median_rank_ratio': 0.1,\n",
       " 'mean_rank': 8.559040590405903,\n",
       " 'median_rank': 5.0,\n",
       " 'mean_probability': 0.0991734128641277,\n",
       " 'median_probability': 0.06358249120774245,\n",
       " 'mean_probability_diff': -0.09017211150846485,\n",
       " 'median_probability_diff': -0.0913922603824946,\n",
       " 'log_likelihood': -1564.1135301674294,\n",
       " 'mean_log_likelihood': -2.885818321342121,\n",
       " 'AIC': 3162.227060334859}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_KPIs(test_stats, len(TRAIN_CONFIG['MNL_features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "\n",
    "In this section, we show some examples on how to use the APIs to serialize the model and eventually deploy it in the production environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deptime_inbound_cos2p': -0.8496660580070212,\n",
       " 'deptime_inbound_cos4p': -0.30762897420538404,\n",
       " 'deptime_inbound_sin2p': -1.6232614842772368,\n",
       " 'deptime_inbound_sin4p': -0.6261308533929246,\n",
       " 'deptime_outbound_cos2p': -1.1658988603880276,\n",
       " 'deptime_outbound_cos4p': -0.1422746541040992,\n",
       " 'deptime_outbound_sin2p': 0.14962669318119354,\n",
       " 'deptime_outbound_sin4p': -0.6681710940490121,\n",
       " 'price_elasticity': 0.0010235259234092533,\n",
       " 'reco_contains_CX': 1.2553528481302576,\n",
       " 'reco_contains_MH': -2.243281322063111,\n",
       " 'reco_contains_OD': -1.4806246614161545,\n",
       " 'reco_contains_PG': -14.88035895891395,\n",
       " 'reco_contains_SQ': 0.9626827764243119,\n",
       " 'reco_contains_TG': 0.6494038674170384,\n",
       " 'reco_contains_VN': -2.5550744181940255,\n",
       " 'rescaled_reco_eft': -0.7576713365597603}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer the trained model to a minimized model for later inference.\n",
    "\n",
    "model_to_deploy = Mint(TRAIN_CONFIG['MNL_features'], model.get_feature_weights())\n",
    "\n",
    "model_to_deploy.get_feature_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model to  model/mint_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Dump/Pickle the model object into a binary file.\n",
    "model_to_deploy.save('model/mint_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from  model/mint_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# instantialize a new model from the pickle file\n",
    "inference_model = load_model('model/mint_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deptime_inbound_cos2p</th>\n",
       "      <th>deptime_inbound_cos4p</th>\n",
       "      <th>deptime_inbound_sin2p</th>\n",
       "      <th>deptime_inbound_sin4p</th>\n",
       "      <th>deptime_outbound_cos2p</th>\n",
       "      <th>deptime_outbound_cos4p</th>\n",
       "      <th>deptime_outbound_sin2p</th>\n",
       "      <th>deptime_outbound_sin4p</th>\n",
       "      <th>price_elasticity</th>\n",
       "      <th>reco_contains_CX</th>\n",
       "      <th>reco_contains_MH</th>\n",
       "      <th>reco_contains_OD</th>\n",
       "      <th>reco_contains_PG</th>\n",
       "      <th>reco_contains_SQ</th>\n",
       "      <th>reco_contains_TG</th>\n",
       "      <th>reco_contains_VN</th>\n",
       "      <th>rescaled_reco_eft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>0.422618</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>-0.906308</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>0.906308</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>248.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>-0.980785</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.195090</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>-0.994056</td>\n",
       "      <td>0.976296</td>\n",
       "      <td>-0.108867</td>\n",
       "      <td>0.216439</td>\n",
       "      <td>217.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>-0.980785</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.195090</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>-0.518773</td>\n",
       "      <td>-0.461748</td>\n",
       "      <td>-0.854912</td>\n",
       "      <td>0.887011</td>\n",
       "      <td>217.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>-0.980785</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.195090</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>-0.422618</td>\n",
       "      <td>-0.642788</td>\n",
       "      <td>0.906308</td>\n",
       "      <td>-0.766044</td>\n",
       "      <td>233.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>-0.887011</td>\n",
       "      <td>0.573576</td>\n",
       "      <td>-0.461749</td>\n",
       "      <td>0.819152</td>\n",
       "      <td>-0.994056</td>\n",
       "      <td>0.976296</td>\n",
       "      <td>-0.108867</td>\n",
       "      <td>0.216439</td>\n",
       "      <td>233.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      deptime_inbound_cos2p  deptime_inbound_cos4p  deptime_inbound_sin2p  \\\n",
       "1617               0.422618              -0.642788              -0.906308   \n",
       "1618              -0.980785               0.923880               0.195090   \n",
       "1619              -0.980785               0.923880               0.195090   \n",
       "1620              -0.980785               0.923880               0.195090   \n",
       "1621              -0.887011               0.573576              -0.461749   \n",
       "\n",
       "      deptime_inbound_sin4p  deptime_outbound_cos2p  deptime_outbound_cos4p  \\\n",
       "1617              -0.766044               -0.422618               -0.642788   \n",
       "1618              -0.382683               -0.994056                0.976296   \n",
       "1619              -0.382683               -0.518773               -0.461748   \n",
       "1620              -0.382683               -0.422618               -0.642788   \n",
       "1621               0.819152               -0.994056                0.976296   \n",
       "\n",
       "      deptime_outbound_sin2p  deptime_outbound_sin4p  price_elasticity  \\\n",
       "1617                0.906308               -0.766044             248.6   \n",
       "1618               -0.108867                0.216439             217.6   \n",
       "1619               -0.854912                0.887011             217.6   \n",
       "1620                0.906308               -0.766044             233.6   \n",
       "1621               -0.108867                0.216439             233.6   \n",
       "\n",
       "      reco_contains_CX  reco_contains_MH  reco_contains_OD  reco_contains_PG  \\\n",
       "1617                 0                 0                 0                 0   \n",
       "1618                 0                 0                 0                 0   \n",
       "1619                 0                 0                 0                 0   \n",
       "1620                 0                 0                 0                 0   \n",
       "1621                 0                 0                 0                 0   \n",
       "\n",
       "      reco_contains_SQ  reco_contains_TG  reco_contains_VN  rescaled_reco_eft  \n",
       "1617                 0                 1                 0                0.0  \n",
       "1618                 0                 1                 0                0.0  \n",
       "1619                 0                 1                 0                0.0  \n",
       "1620                 0                 1                 0                0.0  \n",
       "1621                 0                 1                 0                0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a session for test\n",
    "sample_session = df_test[df_test['session_id'] == 170326817]\n",
    "test_X = sample_session[TRAIN_CONFIG['MNL_features']]\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.77555756e-17,  3.46944695e-18,  1.73472348e-18,  6.93889390e-18,\n",
       "        6.93889390e-18,  2.77555756e-17,  3.46944695e-18, -6.93889390e-18,\n",
       "        1.38777878e-17,  3.46944695e-18,  8.67361738e-19,  1.38777878e-17,\n",
       "        6.93889390e-18,  3.46944695e-18,  3.46944695e-18,  3.46944695e-18,\n",
       "        2.77555756e-17,  3.46944695e-18,  6.93889390e-18,  3.46944695e-18,\n",
       "        6.93889390e-18,  3.30872245e-24,  5.16987883e-26,  3.23117427e-27,\n",
       "        8.67361738e-19,  8.67361738e-19,  6.93889390e-18,  3.46944695e-18,\n",
       "        6.61744490e-24,  1.03397577e-25,  6.46234854e-27,  8.47032947e-22,\n",
       "        1.69406589e-21,  3.38813179e-21,  6.77626358e-21,  8.47032947e-22,\n",
       "        4.23516474e-22,  1.69406589e-21,  8.47032947e-22,  6.77626358e-21,\n",
       "        4.23516474e-22,  3.38813179e-21,  3.38813179e-21,  6.77626358e-21,\n",
       "        8.47032947e-22,  3.38813179e-21,  1.69406589e-21,  4.23516474e-22,\n",
       "        0.00000000e+00,  6.77626358e-21,  1.69406589e-21,  3.38813179e-21,\n",
       "        2.11758237e-22,  3.38813179e-21,  6.77626358e-21,  3.38813179e-21,\n",
       "        8.47032947e-22,  2.11758237e-22,  3.38813179e-21,  2.11758237e-22])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the results given by the original model with Pytorch and\n",
    "#   the inference model without pytorch. \n",
    "\n",
    "# Note: there are some subtle differences which should be due to the fact\n",
    "#   that the precision provided by Pytorch and Pandas/Numpy libraries is different.\n",
    "inference_model.predict(test_X) - model.predict(test_X).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CONFIG = {\n",
    "    #'MNL_features': MNL_features,\n",
    "    \n",
    "    # when absent, by default, use all the features within the training data\n",
    "    #'alter_features': MNL_features,\n",
    "    #'session_features'\n",
    "    'alter_features':['reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od'],\n",
    "    'session_features':['deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p'],\n",
    "    'choice_groups':['group','group2'],\n",
    "    \n",
    "    # options: BinaryCrossEntropy, MaxLogLikelihood\n",
    "    #'loss':  'MaxLogLikelihood',\n",
    "    'loss':  'BinaryCrossEntropy',\n",
    "    \n",
    "    'expand': True,\n",
    "    \n",
    "    'optimizer': 'Adam',  # options:  Adam, RMSprop, SGD, LBFGS.\n",
    "    # Adam would converge much faster\n",
    "    # LBFGS is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes).\n",
    "    # If it doesnâ€™t fit in memory try reducing the history size, or use a different algorithm.\n",
    "    # By default, history_size == 100\n",
    "    'learning_rate': 1e-3, # Applicable to Adam, SGD, and LBFGS\n",
    "    # The learning_rate parameter seems essential to LBFGS, which converges in two epochs.\n",
    "    #  So far, learning_rate == 0.1 seems to be ok for LBFGS\n",
    "    \n",
    "    #'momentum': 0.9,  # applicable to SGD, RMSprop\n",
    "    'momentum': 0,  # applicable to SGD, RMSprop\n",
    "    \n",
    "    # The resulting model seems to be more balanced, i.e. no extreme large/small weights,\n",
    "    #  although one might not have the most ideal performance, i.e. high top_5_rank etc.\n",
    "    'weight_decay': 0, # Applicable to Adam, RMSprop and SGD\n",
    "    \n",
    "    'epochs': 20,\n",
    "    'early_stop_min_delta': 1e-4,\n",
    "    'patience': 5,\n",
    "    \n",
    "    'gpu': False,  # luckily, running on GPU is faster than CPU in this case.\n",
    "    \n",
    "    # level of logging, 0: no log,  1: print epoch related logs;  2: print session related logs\n",
    "    'verbose': 2,\n",
    "    \n",
    "    # Adding the regularization degredates the performance of model\n",
    "    #   which might suggests that the model is still underfitting, not overfitting.\n",
    "    'l1_loss_weight': 0,  # e.g. 0.001 the regularization that would marginalize the weights\n",
    "    'l2_loss_weight': 0,\n",
    "    \n",
    "    # flag indicates whether to save gradients during the training\n",
    "    'save_gradients': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "alter_data = pd.read_csv('data/train_SINBKK_RT_B_alter_id.csv')\n",
    "session_data = pd.read_csv('data/train_SINBKK_RT_B_session_id.csv')\n",
    "choices = pd.read_csv('data/train_SINBKK_RT_B_choices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num features: 15\n",
      "========================\n",
      "{'alter_features': ['reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od'], 'session_features': ['deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p'], 'choice_groups': ['group', 'group2'], 'loss': 'BinaryCrossEntropy', 'expand': True, 'optimizer': 'Adam', 'learning_rate': 0.001, 'momentum': 0, 'weight_decay': 0, 'epochs': 20, 'early_stop_min_delta': 0.0001, 'patience': 5, 'gpu': False, 'verbose': 1, 'l1_loss_weight': 0, 'l2_loss_weight': 0, 'save_gradients': False, 'MNL_features': ['deptime_outbound_sin2p', 'deptime_outbound_sin4p', 'deptime_outbound_cos2p', 'deptime_outbound_cos4p', 'deptime_inbound_sin2p', 'deptime_inbound_sin4p', 'deptime_inbound_cos2p', 'deptime_inbound_cos4p', 'reco_contains_mh', 'reco_contains_tg', 'reco_contains_pg', 'reco_contains_sq', 'reco_contains_vn', 'reco_contains_cx', 'reco_contains_od'], 'alternatives': {\"[1, 'hola']\": array([32, 31, 27, 41, 20, 55, 11, 25, 37,  4, 17, 58, 18, 22, 33, 44, 36,\n",
      "       52, 26, 39, 42, 49, 28,  6, 59, 47, 40, 54,  7,  3, 30, 57, 15, 16,\n",
      "       50,  9,  8,  0, 14, 45, 35, 19, 56, 24,  1, 29,  2, 48, 34, 46, 12,\n",
      "        5, 43, 23, 21, 38, 53, 51, 10, 13]), \"[2, 'hola']\": array([ 44,  53, 109,   9, 107,  16,  96,  41,  26,  59,  28,  39,  58,\n",
      "        42, 114,  27,  85, 116,  71,  55,  88,  17,  43,  65,  24,  99,\n",
      "       110,  67, 101,  82,  49,  97,  57,  32,  37,  73,  30,  79, 118,\n",
      "        11,  74,  62,  13, 111,  61, 100,  89,  93,  19,   8,  12,  81,\n",
      "        70,  90,  23,  51,   1, 112,  25,  52,  95,  50,  75,  10,  34,\n",
      "        36, 115,  15, 108,   3, 106,  86,  68,  35,  29,  87,  60,  47,\n",
      "        76,   5,  77,  69,   2,   6,  20,  31,   7,  14,  48,  78,  98,\n",
      "        40, 104, 117,   0, 103,  94,  18,  38,  91,  66, 119,  64,  83,\n",
      "        33,   4,  22,  80,  56,  92,  72,  63,  21,  46, 113, 102,  54,\n",
      "        84,  45, 105]), \"[4, 'hola']\": array([54, 10, 34, 19, 50, 58, 42, 27, 30, 28, 15,  8, 49, 21, 47, 56, 45,\n",
      "       23, 59, 48, 53, 35, 38, 36, 39,  5,  6, 14,  0, 33, 51,  2, 46, 43,\n",
      "        9, 12, 17,  1, 41, 57, 37, 44, 18, 32, 31, 20, 52, 29,  4, 11,  3,\n",
      "       13, 25, 22, 24, 26, 55, 16,  7, 40]), \"[3, 'hola']\": array([5, 7, 9, 6, 4, 1, 3, 0, 8, 2])}}\n",
      "========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n",
      "/home/jovyan/UofT/Ideas/location/code/nmlogit/MNL.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.t(self.softmax(util_values))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got nan at /opt/conda/conda-bld/pytorch_1579022027550/work/aten/src/THNN/generic/BCECriterion.c:62",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-321-7bff66bcdf09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_CONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malter_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malter_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/UofT/Ideas/location/code/nmlogit/MNL_plus.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(df_training, train_config, alter_data, session_data, model_tuple)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mdf_session_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'session_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         loss_list = train_with_early_stopping(model_tuple=model_tuple, train_data=df_session_groups, train_config=train_config,\n\u001b[0m\u001b[1;32m    341\u001b[0m                                               alter_data=alter_data, session_data=session_data)\n\u001b[1;32m    342\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UofT/Ideas/location/code/nmlogit/MNL_plus.py\u001b[0m in \u001b[0;36mtrain_with_early_stopping\u001b[0;34m(model_tuple, train_data, alter_data, session_data, train_config)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             epoch_loss = train_one_epoch(epoch_index=epoch, module_tuple=model_tuple, df_session_groups=train_data, train_config=train_config,\n\u001b[0m\u001b[1;32m    221\u001b[0m                                          alter_data=alter_data, session_data=session_data)\n\u001b[1;32m    222\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UofT/Ideas/location/code/nmlogit/MNL_plus.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, module_tuple, df_session_groups, alter_data, session_data, train_config)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             cost = model.train(loss, optimizer,\n\u001b[0m\u001b[1;32m    170\u001b[0m                                \u001b[0mdf_session\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMNL_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                                \u001b[0mdf_session\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choice'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/UofT/Ideas/location/code/nmlogit/MNL.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loss, optimizer, x_val, y_val, l1_loss_weight, l2_loss_weight, gpu)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Forward to calculate the losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# optional: add L1 or L2 penalities for regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2076\u001b[0;31m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[1;32m   2077\u001b[0m         input, target, weight, reduction_enum)\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `x >= 0. && x <= 1.' failed. input value should be between 0~1, but got nan at /opt/conda/conda-bld/pytorch_1579022027550/work/aten/src/THNN/generic/BCECriterion.c:62"
     ]
    }
   ],
   "source": [
    "model_tuple, loss_list = run_training(df_training=choices, train_config=TRAIN_CONFIG, alter_data=alter_data, session_data=session_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alternatives = TRAIN_CONFIG.get('alternatives',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"[1, 'hola']\": array([32, 31, 27, 41, 20, 55, 11, 25, 37,  4, 17, 58, 18, 22, 33, 44, 36,\n",
       "        52, 26, 39, 42, 49, 28,  6, 59, 47, 40, 54,  7,  3, 30, 57, 15, 16,\n",
       "        50,  9,  8,  0, 14, 45, 35, 19, 56, 24,  1, 29,  2, 48, 34, 46, 12,\n",
       "         5, 43, 23, 21, 38, 53, 51, 10, 13]),\n",
       " \"[2, 'hola']\": array([ 44,  53, 109,   9, 107,  16,  96,  41,  26,  59,  28,  39,  58,\n",
       "         42, 114,  27,  85, 116,  71,  55,  88,  17,  43,  65,  24,  99,\n",
       "        110,  67, 101,  82,  49,  97,  57,  32,  37,  73,  30,  79, 118,\n",
       "         11,  74,  62,  13, 111,  61, 100,  89,  93,  19,   8,  12,  81,\n",
       "         70,  90,  23,  51,   1, 112,  25,  52,  95,  50,  75,  10,  34,\n",
       "         36, 115,  15, 108,   3, 106,  86,  68,  35,  29,  87,  60,  47,\n",
       "         76,   5,  77,  69,   2,   6,  20,  31,   7,  14,  48,  78,  98,\n",
       "         40, 104, 117,   0, 103,  94,  18,  38,  91,  66, 119,  64,  83,\n",
       "         33,   4,  22,  80,  56,  92,  72,  63,  21,  46, 113, 102,  54,\n",
       "         84,  45, 105]),\n",
       " \"[4, 'hola']\": array([54, 10, 34, 19, 50, 58, 42, 27, 30, 28, 15,  8, 49, 21, 47, 56, 45,\n",
       "        23, 59, 48, 53, 35, 38, 36, 39,  5,  6, 14,  0, 33, 51,  2, 46, 43,\n",
       "         9, 12, 17,  1, 41, 57, 37, 44, 18, 32, 31, 20, 52, 29,  4, 11,  3,\n",
       "        13, 25, 22, 24, 26, 55, 16,  7, 40]),\n",
       " \"[3, 'hola']\": array([5, 7, 9, 6, 4, 1, 3, 0, 8, 2])}"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_groups.extend(['alter_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['group', 'group2', 'alter_id', 'alter_id', 'alter_id']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 58, 52, 39, 18, 46, 20,  9, 50,  7, 30, 43, 35, 42, 25, 33, 44,\n",
       "       37, 15, 11,  0, 12, 17, 36,  3,  5, 59,  6, 55, 34, 10, 40, 49, 21,\n",
       "        2,  1, 47, 26,  4, 54, 27, 56, 14, 22, 51, 13, 41, 16, 32, 28,  8,\n",
       "       23, 48, 19, 57, 45, 24, 29, 38, 31])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_alternatives[repr([1,'hola'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = choices[choice_groups].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>group2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>2</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3894</th>\n",
       "      <td>4</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>3</td>\n",
       "      <td>hola</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group group2\n",
       "0         1   hola\n",
       "1947      2   hola\n",
       "3894      4   hola\n",
       "5841      3   hola"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(unique_values.iloc[groupcombo][var])==str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'hola']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "aunique_values = choices[choice_groups].drop_duplicates()\n",
    "alternatives = {}\n",
    "for groupcombo in range(0,len(unique_values)):\n",
    "    cond = ''\n",
    "    for var in choice_groups:\n",
    "        equalto = unique_values.iloc[groupcombo][var]\n",
    "        if type(equalto)==str:\n",
    "            equalto = '\"' + equalto + '\"'\n",
    "        if cond == '':\n",
    "            cond = cond + var + '==' + str(equalto)\n",
    "        else:\n",
    "            cond = cond + ' & ' + var + '==' + str(equalto)\n",
    "    # this assumes that all neighborhoods with no firm are not part of the choice set\n",
    "    alternatives[repr(df_training.iloc[groupcombo].values.tolist())] = choices.query(cond).alter_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"[1, 'hola']\": array([53, 58, 52, 39, 18, 46, 20,  9, 50,  7, 30, 43, 35, 42, 25, 33, 44,\n",
       "        37, 15, 11,  0, 12, 17, 36,  3,  5, 59,  6, 55, 34, 10, 40, 49, 21,\n",
       "         2,  1, 47, 26,  4, 54, 27, 56, 14, 22, 51, 13, 41, 16, 32, 28,  8,\n",
       "        23, 48, 19, 57, 45, 24, 29, 38, 31]),\n",
       " \"[2, 'hola']\": array([ 59, 108,  63,  48,  91,  17,  37, 118,  93,  71, 111, 104,  28,\n",
       "         12,  94,  29, 107,  44,  85, 115,  99,  23,  32,  83, 106,  53,\n",
       "         87,  88, 109,  89,  81,  21, 101, 114,  42,  64,  43,  75,  52,\n",
       "         47,   9, 117,  20,   5,  30,  65,  97,  72,  18,  38,  25,  73,\n",
       "         19,  41,  69,   7, 100,  55,  31,  70,  40,  58,  60,  67,  74,\n",
       "         11,  62,  16,  76,   1,  36,   8,  46, 102, 112,   6,  82,  39,\n",
       "         15,  26,  33,  51,  92,   2,   0,  95,  49,  14,  35, 116,  57,\n",
       "        119, 105,  96,  24,  34,  68,   3,  86,  79,  80,  22, 113,  56,\n",
       "         13,  66,   4,  27,  54,  77,  78, 110, 103,  98,  84,  45,  50,\n",
       "         61,  90,  10]),\n",
       " \"[4, 'hola']\": array([57, 38,  2, 49, 28, 47, 45, 10,  4, 51, 41, 44,  8, 37, 50, 16, 58,\n",
       "        35, 25,  6, 12, 29, 55, 19, 30, 48, 20,  7, 56, 23, 22,  0, 40, 15,\n",
       "        33, 24, 46, 27,  3, 39, 17, 43,  1, 34, 11, 14, 32, 21, 42, 53,  9,\n",
       "        59, 36, 13, 31, 18,  5, 52, 54, 26]),\n",
       " \"[3, 'hola']\": array([6, 3, 5, 9, 8, 2, 4, 0, 1, 7])}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = choices[['group']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3894</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group\n",
       "0         1\n",
       "1947      2\n",
       "3894      4\n",
       "5841      3"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for group in unique_values:\n",
    "            cond = ''\n",
    "            for var in choice_groups:\n",
    "                if cond == '':\n",
    "                    cond = cond + var + '==' + str(unique_values[group].values[0])\n",
    "                else:\n",
    "                    cond = cond + ' & ' + var + '==' + str(unique_values[group].values[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for groupcombo in unique_values:\n",
    "            cond = ''\n",
    "            for var in choice_groups:\n",
    "                if cond == '':\n",
    "                    cond = cond + var + '==' + str(unique_values[groupcombo].values[0])\n",
    "                else:\n",
    "                    cond = cond + ' & ' + var + '==' + str(unique_values[groupcombo].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternatives = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "for groupcombo in range(0,len(unique_values)):\n",
    "    cond = ''\n",
    "    for var in choice_groups:\n",
    "        if cond == '':\n",
    "            cond = cond + var + '==' + str(unique_values.iloc[groupcombo].values[0])\n",
    "        else:\n",
    "            cond = cond + ' & ' + var + '==' + str(unique_values.iloc[groupcombo].values[0])\n",
    "    # this assumes that all neighborhoods with no firm are not part of the choice set\n",
    "    alternatives[groupcombo] = choices.query(cond).alter_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group': array([ 4,  6, 17, 13, 27, 54, 25, 39, 24, 42, 19, 53, 10, 59, 58, 38,  8,\n",
       "        57,  5, 30,  7, 18, 21, 23, 36, 56, 41, 34, 32, 15, 40, 46, 43, 49,\n",
       "        55,  3, 37, 35, 51, 28, 44, 31,  9, 29,  0, 48, 14, 26,  1, 16, 11,\n",
       "        20, 12,  2, 47, 50, 22, 33, 45, 52]),\n",
       " 0: array([ 4,  6, 17, 13, 27, 54, 25, 39, 24, 42, 19, 53, 10, 59, 58, 38,  8,\n",
       "        57,  5, 30,  7, 18, 21, 23, 36, 56, 41, 34, 32, 15, 40, 46, 43, 49,\n",
       "        55,  3, 37, 35, 51, 28, 44, 31,  9, 29,  0, 48, 14, 26,  1, 16, 11,\n",
       "        20, 12,  2, 47, 50, 22, 33, 45, 52]),\n",
       " 1: array([ 24,  41, 111,  80,  46, 106,  48, 110,  90,  76,   4,   1,  42,\n",
       "         56,  21,  98,  31,  51,  35,  79,  55,  17,  25,  11,  43,  52,\n",
       "         68, 113,  84,   0,  50,  59,  20,  29,  91,   7,  13, 119,  85,\n",
       "        112,  39,   5,  28,  93,  33, 118,  15,  32,  99,  58,  72,  63,\n",
       "        117,  36, 101,  23,   8,   2,  54,  53,  37,  66,  62, 114,  81,\n",
       "         38,  70,  34,  14,  44,  89,  47,  87,   9,  65, 100,  26, 105,\n",
       "         19, 103,   3,  74,  49, 108,  40,  10,  97,  27,  73, 102,  60,\n",
       "         88,  16,  57,  18,  61,  77,  83,  30,  45,  69,  67, 104,  64,\n",
       "         86,  71,  75,  95,  94, 107, 109,  12, 116,  78,  92,   6,  82,\n",
       "         22,  96, 115]),\n",
       " 2: array([52,  2, 28, 20, 29, 14, 38, 47, 50, 33, 51, 42, 11, 18, 36, 25, 23,\n",
       "        24, 37, 34, 16, 26, 17, 27, 59,  8,  7, 55, 10, 32,  4, 57, 48, 12,\n",
       "        13, 39,  3, 41, 30, 15, 45, 46, 22, 35,  9,  1, 43, 21, 54, 44, 19,\n",
       "         0, 53,  6, 56, 31, 40, 49,  5, 58]),\n",
       " 3: array([7, 9, 5, 3, 4, 6, 8, 0, 2, 1])}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
